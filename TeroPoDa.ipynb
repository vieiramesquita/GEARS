{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YrBeM9Mq0JCK",
        "XgrzaUwl_qSY",
        "KOazO2JI-X3p"
      ],
      "authorship_tag": "ABX9TyOfJWvD+axW03vQFrE9K5w8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vieiramesquita/GEARS/blob/master/TeroPoDa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Name\n",
        "TeroPoDa - Time Series Extraction for Polygonal Data\n",
        "\n",
        "### Description\n",
        "Toolkit created to extract time series information from Sentinel 2 data stored in Earth Engine\n",
        "\n",
        "### Author\n",
        "  Vin√≠cius Vieira Mesquita - vieiramesquita@gmail.com\n",
        "\n",
        "### Version\n",
        "  1.0.5"
      ],
      "metadata": {
        "id": "8ZN0zzrtx5BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import main libraries\n",
        "\n",
        "Run the following cell to import the main API's into your session."
      ],
      "metadata": {
        "id": "MMa2J7N_G-ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import ee\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from joblib import Parallel, delayed"
      ],
      "metadata": {
        "id": "j332acITGpEz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate and initialize\n",
        "\n",
        "Run the `ee.Authenticate` function to authenticate your access to Earth Engine servers and `ee.Initialize` to initialize it. Upon running the following cell you'll be asked to grant Earth Engine access to your Google account. Follow the instructions printed to the cell."
      ],
      "metadata": {
        "id": "4Dr-6br_M9tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigger the authentication flow.\n",
        "#ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()\n"
      ],
      "metadata": {
        "id": "RX26uO4nM7s0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the NDVI Time Series from Earth Engine \n",
        "\n",
        "Function responsible to get the time series of Sentinel 2 data throught Earth Engine.\n",
        "\n",
        "This function needs a `geometry` object in the `ee.Feature()` formart and the choosed vetor propertie ID as the `id_field`.\n",
        "\n"
      ],
      "metadata": {
        "id": "29j9yGY3SPr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTimeSeries(geometry,id_field,bestEffort=False):\n",
        "  \n",
        "  def mask_img(img):\n",
        "\n",
        "    satName = ee.String(img.get('SPACECRAFT_NAME'))\n",
        "  \n",
        "    maskExp = \"(b('MSK_CLDPRB') < 5 && b('SCL') != 3 && b('SCL') != 10)\"\n",
        "\n",
        "    #maskExp = \"b('Q60') == 0\"\n",
        "\n",
        "    mask = img.expression(maskExp).add(img.lte(0));\n",
        "\n",
        "    ndvi = img.updateMask(mask).normalizedDifference(['B8','B4']).select([0],['NDVI'])\n",
        "  \n",
        "    return (img.addBands(ndvi, None, True)\n",
        "      .set({'system:time_start':img.get('system:time_start'),'satelite':satName}))\n",
        "\n",
        "  def reduceData(img):\n",
        "\n",
        "    img = ee.Image(img)\n",
        "\n",
        "    imgDate = ee.Date(ee.Number(img.get('system:time_start')))\n",
        "    orgDate = (ee.String(ee.Number(imgDate.get('year')).toInt().format())\n",
        "      .cat('-')\n",
        "      .cat(ee.String(ee.Number(imgDate.get('month')).toInt().format()))\n",
        "      .cat('-')\n",
        "      .cat(ee.String(ee.Number(imgDate.get('day')).toInt().format()))\n",
        "      )\n",
        "    \n",
        "    reducers = (ee.Reducer.mean()\n",
        "        .combine(**{'reducer2': ee.Reducer.stdDev(),'sharedInputs':True,})\n",
        "        .combine(**{'reducer2': ee.Reducer.count(),'sharedInputs':True}))\n",
        "\n",
        "    if bestEffort == False: \n",
        "      series = img.reduceRegion(reducers,ee.Feature(geometry).geometry(), 10,None,None,False,1e13,16)\n",
        "    else:\n",
        "      series = img.reduceRegion(reducers,ee.Feature(geometry).geometry(), None,None,None,True,1e13,16)\n",
        "    \n",
        "    return (ee.Feature(geometry)\n",
        "      .set('id',ee.String(img.id()))\n",
        "      .set('date',orgDate)\n",
        "      .set('satelite',img.get('satelite'))\n",
        "      .set('MGRS_TILE',img.get('MGRS_TILE'))      \n",
        "      .set('AREA_HA',ee.Feature(geometry).get('AREA_HA'))\n",
        "      .set(id_field,ee.Feature(geometry).get(id_field))\n",
        "      .set('NDVI_mean',ee.Number(ee.Dictionary(series).get('NDVI_mean')))\n",
        "      .set('NDVI_stdDev',ee.Number(ee.Dictionary(series).get('NDVI_stdDev')))\n",
        "      .set('Pixel_Count',ee.Number(ee.Dictionary(series).get('NDVI_count')))\n",
        "      #.toDictionary()\n",
        "    )\n",
        "\n",
        "  def toDict(feat):\n",
        "    return ee.Feature(feat).toDictionary()\n",
        "\n",
        "  #imgCol =  (ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\").filterBounds(geometry.geometry())\n",
        "  #  .map(mask_img))\n",
        "\n",
        "  imgCol = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "    .filterBounds(geometry.geometry())\n",
        "    .map(mask_img))\n",
        "\n",
        "  Coll_fill = (imgCol.toList(imgCol.size()).map(reduceData)\n",
        "    .filter(ee.Filter.notNull(['NDVI_mean']))\n",
        "    .map(toDict)\n",
        "    )\n",
        "    \n",
        "  return Coll_fill"
      ],
      "metadata": {
        "id": "-xI_LefsSPgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and Structure the Time Series library\n",
        "\n",
        "Function responsible to build and structure the time series library."
      ],
      "metadata": {
        "id": "YrBeM9Mq0JCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_time_series(index,obj,id_field,outfile,asset,bestEffort=False):\n",
        "\n",
        "  samples = ee.FeatureCollection(asset).select(id_field)\n",
        "  #samples = ee.FeatureCollection('users/vieiramesquita/TNC_Vectors/monitoring_polygons_TNC').select(id_field)\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  start_time_obj = time.time()\n",
        "\n",
        "  samples_split = samples.filter(ee.Filter.eq(id_field,obj)).first()\n",
        "\n",
        "  point_series = getTimeSeries(ee.Feature(samples_split),id_field,bestEffort).getInfo()\n",
        "  \n",
        "  for item in point_series:\n",
        "    df  = pd.concat([df,pd.DataFrame(item,index=[0])])\n",
        "  \n",
        "  hdr = False if os.path.isfile(outfile) else True\n",
        "  \n",
        "  df['NDVI_mean'] = df['NDVI_mean'].round(decimals=4) \n",
        "  df['NDVI_stdDev'] = df['NDVI_stdDev'].round(decimals=4) \n",
        "\n",
        "  df.to_csv(outfile,mode='a',header = hdr,index = False,sep =';')\n",
        "\n",
        "  time_spended = round(time.time() - start_time_obj, 3)\n",
        "\n",
        "  print(f'Index {index} - Object [{obj}] procesed in {round(time.time() - start_time_obj, 3)} seconds')\n",
        "\n",
        "  if df.shape[0] > 0: \n",
        "    return True,time_spended\n",
        "  elif float(df['AREA_HA']) < 0.01:\n",
        "    return None,None\n",
        "  else:\n",
        "    return False,None"
      ],
      "metadata": {
        "id": "1n_GSXfk0IvV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the Time Series library\n",
        "\n",
        "Function responsible to check the consistency of the time series library."
      ],
      "metadata": {
        "id": "XgrzaUwl_qSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_time_series_reroll(index,obj,id_field,outfile,asset,checker=False):\n",
        "\n",
        "  obj = int(obj)\n",
        "\n",
        "  if checker is True:\n",
        "\n",
        "    df_check = pd.read_csv(outfile,sep =';')\n",
        "\n",
        "    if obj in list(df_check[id_field].values):\n",
        "\n",
        "      print(f' Object [{obj}] was found in the file. Skipping..')\n",
        "      return {'errors':None, 'time': 0}\n",
        "\n",
        "  errors = None\n",
        "  time = None\n",
        "\n",
        "  try:\n",
        "    check = build_time_series(index,obj,id_field,outfile,asset)\n",
        "    time = check[1]\n",
        "\n",
        "    if check[0] == False:\n",
        "      print('raised')\n",
        "      raise\n",
        "    if check[0] == None:\n",
        "      return {'errors':'ignore' ,'time': 'ignore'}\n",
        "      \n",
        "  except:\n",
        "\n",
        "    try:\n",
        "\n",
        "      print(f'Index {index} - Request [{obj}] fails. Trying the best effort!')\n",
        "\n",
        "      check = build_time_series(index,obj,id_field,outfile,asset,True)\n",
        "\n",
        "      if check[0] == False:\n",
        "        print('raised')\n",
        "        raise\n",
        "      \n",
        "      if check[0] == None:\n",
        "        return {'errors':'ignore' ,'time': 'ignore'}\n",
        "      \n",
        "    except:\n",
        "\n",
        "      print(f'Index {index} - Request [{obj}] expired. Sending it to the error list!')\n",
        "        \n",
        "      errors = obj\n",
        "\n",
        "  return {'errors':errors ,'time': time}"
      ],
      "metadata": {
        "id": "SpNM9wFuAD1f"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Polygon List file\n",
        "\n",
        "Function responsible to write a text file contaning each Polygon ID used to extract the time series."
      ],
      "metadata": {
        "id": "KOazO2JI-X3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_id_list(asset,id_field):\n",
        "  \n",
        "  samples = ee.FeatureCollection(asset).select(id_field)\n",
        "  \n",
        "  sample_size = int(samples.size().getInfo())\n",
        "\n",
        "  if sample_size > 50000:\n",
        "    samples_list = samples.toList(50000)\n",
        "  else:\n",
        "    samples_list = samples.toList(samples.size())\n",
        "\n",
        "  with open('/content/polygonList.txt', \"w\") as polygon_file:\n",
        "\n",
        "    def get_ids(feat):\n",
        "      return ee.Feature(feat).get(id_field)\n",
        "\n",
        "    samples_list_slice = samples_list.map(get_ids).sort().getInfo()\n",
        "\n",
        "    for polygon in samples_list_slice:\n",
        "      polygon_file.write(str(polygon)+ '\\n')\n",
        "  "
      ],
      "metadata": {
        "id": "WSpenzzJjbcO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run\n",
        "\n",
        "Function responsible to catch argument information and start run the process."
      ],
      "metadata": {
        "id": "C7pp7bb4A40W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(asset,id_field,output_name):\n",
        "  start_time = time.time()\n",
        "\n",
        "  mon_Polygons_text = open(\"/content/polygonList.txt\",\"r\")\n",
        "  mon_Polygons = mon_Polygons_text.readlines()\n",
        "\n",
        "  mon_Polygons = [int(name) for name in mon_Polygons]\n",
        "\n",
        "  start_obj = 1\n",
        "\n",
        "  total = len(mon_Polygons)\n",
        "\n",
        "  print(f'Number of objects to process: {total}')\n",
        "\n",
        "  if total > 1000:\n",
        "    print('Go take a coffee and watch a series... it will take a while!')\n",
        "  \n",
        "  #out_dir = r'E:\\TrabalhosLAPIG\\ToolkitNDVI_S2_TNC'\n",
        "  #output_name = os.path.join(out_dir,'TNC_S2_NDVI_Monitoring.csv')\n",
        "  #id_field = 'ID_POL'\n",
        "\n",
        "  list_num = mon_Polygons[start_obj:total]\n",
        "\n",
        "  #list_num = mon_Polygons[1:50]\n",
        "\n",
        "  first_dict = [{'errors':'ignore' ,'time': 'ignore'}]\n",
        "  check_file = True\n",
        "\n",
        "  if os.path.exists(output_name) is False:\n",
        "\n",
        "    check_file = False\n",
        "    print('Creating the main file..')\n",
        "    first_dict = build_time_series_reroll(0,int(mon_Polygons[0]),id_field,output_name,asset)\n",
        "\n",
        "  worker_args = [\n",
        "    (mon_Polygons.index(obj),obj,id_field,output_name,asset,check_file) \\\n",
        "    for obj in list_num\n",
        "  ]\n",
        "  \n",
        "  infos = Parallel(n_jobs=16, backend='multiprocessing')(delayed(build_time_series_reroll)(*args) for args in worker_args)\n",
        "\n",
        "  if check_file is True:\n",
        "    first_dict = {'time': 0}\n",
        "\n",
        "  time_list = [first_dict['time']] + [item['time'] for item in infos if item['time'] != None]\n",
        "\n",
        "  errors_list = [item['errors'] for item in infos if item['errors'] != None]\n",
        "\n",
        "  with open('/content/errors_polygon.txt', \"w\") as errors_file:  \n",
        "    for polygon in errors_list:\n",
        "      errors_file.write(str(polygon)+ '\\n')\n",
        "\n",
        "  print(f'The average processing time was {round(pd.DataFrame(time_list).mean()[0],2)} seconds')\n",
        "  print(f'Processing finished. All the work took {round(time.time() - start_time,3)} seconds to complete')"
      ],
      "metadata": {
        "id": "ZjWWmlIgDvwb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  asset = 'users/vieiramesquita/TNC_Vectors/monitoring_polygons_TNC'\n",
        "  id_field = 'ID_POL'\n",
        "  output_name = '/content/TNC_S2_NDVI_Monitoring.csv'\n",
        "\n",
        "  if os.path.exists('/content/polygonList.txt') is False:\n",
        "    build_id_list(asset,id_field)\n",
        "\n",
        "  run(asset,id_field,output_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_sbLkobD5Of",
        "outputId": "73919b40-25da-43f2-d18b-caab6c7806b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Index 5107 - Object [10217] procesed in 16.48 seconds\n",
            "Index 5113 - Object [10223] procesed in 6.989 seconds\n",
            "Index 5112 - Object [10222] procesed in 9.957 seconds\n"
          ]
        }
      ]
    }
  ]
}